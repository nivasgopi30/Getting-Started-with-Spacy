{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8bde05-d05c-4a1f-b8a6-aa34ed1d6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b69a9b4-8cd5-4898-a415-bf5f038f9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d748e4fa-2114-4bc7-bdf5-177e4cdf36c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80008da5-99fa-4243-ba4e-00c6017f94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f79c7be9ee0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f79c7be9dc0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f79c7bf52e0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f79c7a05d00>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f79c7995400>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f79c7bf5270>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d51239-2218-4ea8-ae6b-07e78c854e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_of_doc', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This component contains 5 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Creating our own custom component in the nlp pipeline\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component('length_of_doc')\n",
    "def length_comp(doc):\n",
    "    doc_length = len(doc)\n",
    "    print(f'This component contains {doc_length} tokens.')\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('length_of_doc', first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp('This is a sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b905293c-3553-4533-a54f-bc44602979ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print(doc.ents)\n",
    "animals = [\"Golden Retriever\", \"cat\", \"Turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('ANIMALS', animal_patterns)\n",
    "\n",
    "@Language.component('extract_animal')\n",
    "def animal_component(doc):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label='ANIMAL') for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a61dc2-679d-46d7-9a0d-ecd308e8ac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'extract_animal']\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe('extract_animal', after='ner')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52c3952-56a7-4156-af59-edbab1cacd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(cat, Golden Retriever)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7e1761-6396-40f7-a454-4acaec1d47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1c95c4-604a-468d-bb9e-246f3bd4b461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 6303828839600189595), ('Golden Retriever', 6303828839600189595)]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ed4901-90ed-40b4-890c-8c6b4f595261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting extension attributes\n",
    "from spacy.tokens import Token, Span, Doc\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "Token.set_extension(\"is_country\", default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "796e8f78-5894-4895-953a-eb8ed88f70a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I live in Spain.')\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6691cb4a-81b7-4633-b3b7-994d79b29b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting property extension\n",
    "def get_reversed_token(token):\n",
    "    return token.text[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7d99a47-a2c1-4eeb-af41-676dcc1fd576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('All', 'llA'), ('generalizations', 'snoitazilareneg'), ('are', 'era'), ('false', 'eslaf'), (',', ','), ('including', 'gnidulcni'), ('this', 'siht'), ('one', 'eno'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "Token.set_extension('reversed', getter=get_reversed_token, force=True)\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "print([(token.text, token._.reversed) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2b2c6db-1290-4ed5-bce1-7e6b405547ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_has_number(doc):\n",
    "    return any(token.like_num for token in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad2aec82-f2ec-442a-b3fc-0711de1c325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "Doc.set_extension('has_num', getter=get_has_number)\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(doc._.has_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "991494d4-ab40-43e5-bd37-f95fe8081864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "def to_html(span, tag):\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "Span.set_extension('to_html', method=to_html,force=True)\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3417a3dc-9624-4410-b606-a7adec0827e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e4cc3fd-c723-4da7-b762-f0951bac0c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "Span.set_extension('get_url', getter=get_wikipedia_url,force=True)\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent._.get_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "531c3efc-eb96-456a-9af6-a1e7aa38f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./countries.json', encoding='utf8') as file:\n",
    "    COUNTRIES = json.loads(file.read())\n",
    "with open('./capitals.json', encoding = 'utf8') as file:\n",
    "    CAPITALS = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9beca0f6-f9cc-4be1-8e06-d54f117353e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'countries_comp']\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('COUNTRY', list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "@Language.component('countries_comp')\n",
    "def countries_func(doc):\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label='GPE') for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe('countries_comp', after='ner')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "def get_capital(span):\n",
    "    return CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital, force=True)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bac260e-08c4-4185-bbb4-1bc66de56ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[McDonalds is my favorite restaurant.,\n",
       " Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..,\n",
       " People really still eat McDonalds :(,\n",
       " The McDonalds in Spain has chicken wings. My heart is so happy ,\n",
       " @McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P,\n",
       " please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D,\n",
       " This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "with open('./tweets.json',encoding='utf8') as file:\n",
    "    TEXTS = json.loads(file.read())\n",
    "    \n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2f8ffa5-263e-4e06-abee-1608d14032db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open']\n",
      "['terrible']\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4797ef4d-4606-4372-b526-a170b91dd31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "['McDonalds']\n",
      "['McDonalds', 'Spain']\n",
      "['times!!']\n",
      "['McRib SANDWICH SO']\n",
      "['This morning']\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print([ent.text for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f0f5810-c907-466d-b6fc-472478e7e966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[David Bowie, Angela Merkel, Lady Gaga]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "patterns = list(nlp.pipe(people))\n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06ceb958-bea2-4f79-b347-08db686cdedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\n",
      " -Metamorphosis by Franz Kafka\n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing.\n",
      " -Moby-Dick or, The Whale by Herman Melville\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      " -A Tale of Two Cities by Charles Dickens\n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\n",
      " -On the Road by Jack Kerouac\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      " -1984 by George Orwell\n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing.\n",
      " -The Picture Of Dorian Gray by Oscar Wilde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./bookquotes.json') as file:\n",
    "    QUOTES = json.loads(file.read())\n",
    "    \n",
    "nlp = spacy.blank('en')\n",
    "Doc.set_extension('Author',default=None,force=True)\n",
    "Doc.set_extension('Book',default=None,force=True)\n",
    "\n",
    "for doc, context in nlp.pipe(QUOTES,as_tuples=True):\n",
    "    doc._.Author = context['author']\n",
    "    doc._.Book = context['book']\n",
    "    print(f'{doc.text}\\n -{doc._.Book} by {doc._.Author}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98ff9107-fc6d-4890-b998-c84f30651e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chick, -, fil, -, A, is, an, American, fast, food, restaurant, chain, headquartered, in, the, city, of, College, Park, ,, Georgia, ,, specializing, in, chicken, sandwiches, .]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp.make_doc(text) # make_doc method processes the text given into doc by only tokenizing. Here other pipeline \n",
    "                         # components are not involved while converting.\n",
    "\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fa9e340-df51-4d05-b511-f562b811138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chick, American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "with nlp.select_pipes(disable=['tagger', 'lemmatizer']):    \n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "403178dd-0b96-48bc-af02-b0324c6046ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iPhone X,)\n",
      "(iPhone X,)\n",
      "(iPhone X,)\n",
      "(iPhone 8,)\n",
      "(iPhone 11, iPhone 8)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "TEXTS = ['How to preorder the iPhone X', 'iPhone X is coming', 'Should I pay $1,000 for the iPhone X?', 'The iPhone 8 reviews are here', \"iPhone 11 vs iPhone 8: What's the difference?\", 'I need a new phone! Any tips?']\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "pattern1 = [{'LOWER':'iphone'}, {'LOWER':'x'}]\n",
    "pattern2 = [{'LOWER':'iphone'}, {'IS_DIGIT':True}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('GADGET', patterns=[pattern1, pattern2])\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label='GADGET') for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cfe1203-e640-42b6-a1f3-bc9a7c400ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk('./train.spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7e934fd-6bda-4360-ab96-686d1ad73078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;1m✘ The provided output file already exists. To force overwriting the\n",
      "config file, set the --force or -F flag.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init config ./config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52574a5b-c7d0-4cd6-a028-b01f332c7874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\n",
      "train = null\n",
      "dev = null\n",
      "vectors = null\n",
      "init_tok2vec = null\n",
      "\n",
      "[system]\n",
      "gpu_allocator = null\n",
      "seed = 0\n",
      "\n",
      "[nlp]\n",
      "lang = \"en\"\n",
      "pipeline = [\"tok2vec\",\"ner\"]\n",
      "batch_size = 1000\n",
      "disabled = []\n",
      "before_creation = null\n",
      "after_creation = null\n",
      "after_pipeline_creation = null\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
      "\n",
      "[components]\n",
      "\n",
      "[components.ner]\n",
      "factory = \"ner\"\n",
      "incorrect_spans_key = null\n",
      "moves = null\n",
      "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
      "update_with_oracle_cut_size = 100\n",
      "\n",
      "[components.ner.model]\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
      "state_type = \"ner\"\n",
      "extra_state_tokens = false\n",
      "hidden_width = 64\n",
      "maxout_pieces = 2\n",
      "use_upper = true\n",
      "nO = null\n",
      "\n",
      "[components.ner.model.tok2vec]\n",
      "@architectures = \"spacy.Tok2VecListener.v1\"\n",
      "width = ${components.tok2vec.model.encode.width}\n",
      "upstream = \"*\"\n",
      "\n",
      "[components.tok2vec]\n",
      "factory = \"tok2vec\"\n",
      "\n",
      "[components.tok2vec.model]\n",
      "@architectures = \"spacy.Tok2Vec.v2\"\n",
      "\n",
      "[components.tok2vec.model.embed]\n",
      "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
      "width = ${components.tok2vec.model.encode.width}\n",
      "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
      "rows = [5000,2500,2500,2500]\n",
      "include_static_vectors = false\n",
      "\n",
      "[components.tok2vec.model.encode]\n",
      "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
      "width = 96\n",
      "depth = 4\n",
      "window_size = 1\n",
      "maxout_pieces = 3\n",
      "\n",
      "[corpora]\n",
      "\n",
      "[corpora.dev]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.dev}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[corpora.train]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.train}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[training]\n",
      "dev_corpus = \"corpora.dev\"\n",
      "train_corpus = \"corpora.train\"\n",
      "seed = ${system.seed}\n",
      "gpu_allocator = ${system.gpu_allocator}\n",
      "dropout = 0.1\n",
      "accumulate_gradient = 1\n",
      "patience = 1600\n",
      "max_epochs = 0\n",
      "max_steps = 20000\n",
      "eval_frequency = 200\n",
      "frozen_components = []\n",
      "annotating_components = []\n",
      "before_to_disk = null\n",
      "\n",
      "[training.batcher]\n",
      "@batchers = \"spacy.batch_by_words.v1\"\n",
      "discard_oversize = false\n",
      "tolerance = 0.2\n",
      "get_length = null\n",
      "\n",
      "[training.batcher.size]\n",
      "@schedules = \"compounding.v1\"\n",
      "start = 100\n",
      "stop = 1000\n",
      "compound = 1.001\n",
      "t = 0.0\n",
      "\n",
      "[training.logger]\n",
      "@loggers = \"spacy.ConsoleLogger.v1\"\n",
      "progress_bar = false\n",
      "\n",
      "[training.optimizer]\n",
      "@optimizers = \"Adam.v1\"\n",
      "beta1 = 0.9\n",
      "beta2 = 0.999\n",
      "L2_is_weight_decay = true\n",
      "L2 = 0.01\n",
      "grad_clip = 1.0\n",
      "use_averages = false\n",
      "eps = 0.00000001\n",
      "learn_rate = 0.001\n",
      "\n",
      "[training.score_weights]\n",
      "ents_f = 1.0\n",
      "ents_p = 0.0\n",
      "ents_r = 0.0\n",
      "ents_per_type = null\n",
      "\n",
      "[pretraining]\n",
      "\n",
      "[initialize]\n",
      "vectors = ${paths.vectors}\n",
      "init_tok2vec = ${paths.init_tok2vec}\n",
      "vocab_data = null\n",
      "lookups = null\n",
      "before_init = null\n",
      "after_init = null\n",
      "\n",
      "[initialize.components]\n",
      "\n",
      "[initialize.tokenizer]"
     ]
    }
   ],
   "source": [
    "!cat ./config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a1b4389-1b8c-4f2b-85b8-cdc6bac1cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-07-11 11:14:04,182] [INFO] Set up nlp object from config\n",
      "[2022-07-11 11:14:04,192] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-07-11 11:14:04,196] [INFO] Created vocabulary\n",
      "[2022-07-11 11:14:04,197] [INFO] Finished initializing nlp object\n",
      "[2022-07-11 11:14:04,979] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     20.33    1.69    1.04    4.44    0.02\n",
      "  1     200         29.35    993.16   76.92   76.09   77.78    0.77\n",
      "  2     400         72.34    248.20   83.70   81.91   85.56    0.84\n",
      "  4     600         61.40    122.91   82.87   82.42   83.33    0.83\n",
      "  6     800         66.14     89.60   84.75   86.21   83.33    0.85\n",
      "  9    1000        211.15     65.33   83.70   81.91   85.56    0.84\n",
      " 12    1200         78.98     43.77   86.52   87.50   85.56    0.87\n",
      " 16    1400         72.60     28.10   83.80   84.27   83.33    0.84\n",
      " 22    1600        102.35     31.03   84.92   85.39   84.44    0.85\n",
      " 28    1800         56.20     18.74   83.70   81.91   85.56    0.84\n",
      " 36    2000         70.51     17.84   83.43   85.88   81.11    0.83\n",
      " 46    2200         75.50     20.30   84.27   85.23   83.33    0.84\n",
      " 58    2400        124.88     25.61   85.25   83.87   86.67    0.85\n",
      " 70    2600         94.13     18.52   87.64   88.64   86.67    0.88\n",
      " 83    2800          4.28      0.51   85.11   81.63   88.89    0.85\n",
      " 95    3000        198.89     31.45   85.08   84.62   85.56    0.85\n",
      "108    3200         61.39      9.33   88.14   89.66   86.67    0.88\n",
      "120    3400        163.48     19.19   83.15   84.09   82.22    0.83\n",
      "132    3600         54.47     10.25   83.98   83.52   84.44    0.84\n",
      "145    3800        160.12     20.64   87.29   86.81   87.78    0.87\n",
      "157    4000        218.98     31.10   85.87   84.04   87.78    0.86\n",
      "170    4200        587.41     55.43   87.15   87.64   86.67    0.87\n",
      "182    4400         33.41     11.54   87.50   89.53   85.56    0.88\n",
      "194    4600          0.00      0.00   87.29   86.81   87.78    0.87\n",
      "207    4800          0.00      0.00   87.29   86.81   87.78    0.87\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config_gadget.cfg --output ./output --paths.train train_gadget.spacy --paths.dev dev_gadget.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72abf8b6-b067-42d3-bf5d-7ecdaa97cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./output/model-best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc107f21-aa1a-4a8a-9538-c9cbd2579270",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_texts = [\"Apple is slowing down the iPhone 8 and iPhone X - how to stop it\", \"I finally understand what the iPhone X ‘notch’ is for\",\"Everything you need to know about the Samsung Galaxy S9\",\"Looking to compare iPad models? Here’s how the 2018 lineup stacks up\",\n",
    "\"The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple\",\n",
    "\"what is the cheapest ipad, especially ipad pro???\",\n",
    "\"Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "984b8939-54d7-49a6-992a-5e607ec1d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(testing_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "951f9564-b2cf-4365-8070-8c36dd71297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iPhone 8, iPhone X)\n",
      "(iPhone X, ‘notch)\n",
      "(Samsung Galaxy S9,)\n",
      "(iPad,)\n",
      "(iPhone 8, iPhone 8 Plus)\n",
      "(ipad, ipad pro)\n",
      "(Samsung Galaxy, Samsung Electronics)\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1cf7993a-d3f7-49d9-9d88-83971cc8b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "doc1 = nlp(\"i went to amsterdem last year and the canals were beautiful\")\n",
    "doc1.ents = [Span(doc1, 3, 4, label=\"GPE\")]\n",
    "\n",
    "doc2 = nlp(\"You should visit Paris once, but the Eiffel Tower is kinda boring\")\n",
    "doc2.ents = [Span(doc2, 3, 4, label=\"GPE\")]\n",
    "\n",
    "doc3 = nlp(\"There's also a Paris in Arkansas, lol\")\n",
    "doc3.ents = [Span(doc3, 4, 5, label='GPE'), Span(doc3, 6, 7, label = 'GPE')]\n",
    "\n",
    "doc4 = nlp(\"Berlin is perfect for summer holiday: great nightlife and cheap beer!\")\n",
    "doc4.ents = [Span(doc4, 0, 1, label=\"GPE\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7995b4ad-05b7-461b-a7be-de04022c94a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(amsterdem,)\n",
      "(Paris,)\n",
      "(Paris, Arkansas)\n",
      "(Berlin,)\n"
     ]
    }
   ],
   "source": [
    "docs = [doc1, doc2, doc3, doc4]\n",
    "for doc in docs:\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75b91077-90f8-4d58-8b5d-d32159de0250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Reddit, Patreon)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "doc1 = nlp(\"Reddit partners with Patreon to help creators build communities\")\n",
    "doc1.ents = (Span(doc1, 0, 1, label='WEBSITE'), Span(doc1, 3, 4, label='WEBSITE'))\n",
    "doc1.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94f8e2ac-260f-4939-8852-9d687917a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PewDiePie PERSON\n",
      "YouTube WEBSITE\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"PewDiePie smashes YouTube record\")\n",
    "doc2.ents = [Span(doc2, 0, 1, label='PERSON'), Span(doc2, 2, 3, label=\"WEBSITE\")]\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "153f6152-ecce-4fd4-b3fa-eb4c8da98a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit WEBSITE\n",
      "Alexis Ohanian PERSON\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\")\n",
    "doc3.ents = [Span(doc3, 2, 4, label='PERSON'), Span(doc3, 0, 1, label=\"WEBSITE\")]\n",
    "for ent in doc3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
